{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_25068\\582293219.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_ser = pd.to_datetime(df['Initial Source SIT209 Record Date'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Timestamp('2022-12-31 00:00:00'), Timestamp('2024-04-17 00:00:00'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/fire_data.csv')\n",
    "date_ser = pd.to_datetime(df['Initial Source SIT209 Record Date'])\n",
    "start_date, end_date = np.min(date_ser), np.max(date_ser)\n",
    "start_date, end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We recommend that you skip running the below cell, as it makes call to OpenWeatherAPI. The outputs of this cell are contained in the .json files in the data directory, so you may proceed from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_list = list(zip(df['Latitude'], df['Longitude'], date_ser.apply(lambda x: int(x.tz_localize('UTC').timestamp()))))\n",
    "\n",
    "key = 'INSERT API KEY'\n",
    "def coord_to_data(coords):\n",
    "    lat = coords[0]\n",
    "    lon = coords[1]\n",
    "    time = coords[2]\n",
    "    url = f'https://api.openweathermap.org/data/3.0/onecall/timemachine?lat={lat}&lon={lon}&dt={time}&appid={key}'\n",
    "    return requests.get(url).json()\n",
    "\n",
    "json_name = 'data/weather_api_data.json'\n",
    "\n",
    "data_list = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    print(f'Iteration {i}')\n",
    "    if((i % 20 == 0) or (i == (df.shape[0]-1))):\n",
    "        print('Saving Results to JSON...')\n",
    "        with open(json_name, \"w\") as data_file:\n",
    "            json.dump(data_list, data_file, cls=NpEncoder)\n",
    "    try:\n",
    "        data_list.append((df['OBJECTID'].iloc[i], coord_to_data(helper_list[i])))\n",
    "    except ConnectionError as conn:\n",
    "        print(conn)\n",
    "        data_list.append((df['OBJECTID'].iloc[i], None))\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        data_list.append((df['OBJECTID'].iloc[i], None))\n",
    "\n",
    "extra_tuple = [[df['OBJECTID'].iloc[-1], coord_to_data(helper_list[-1])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apologize if this cell is messy - in practice, we ended up saving the raw data to 3 different .json files. \n",
    "# To convert these to DataFrames, we read in each one at a time.\n",
    "# We also missed the last row of the original dataframe, so we append it separately for ease of use.\n",
    "\n",
    "f_1, f_2, f_3 = open('data/weather_api_data.json'), open('data/weather_api_data_2.json'), open('data/weather_api_data_3.json')\n",
    "k_1, k_2, k_3 = json.load(f_1), json.load(f_2), json.load(f_3)\n",
    "k = k_1 + k_2 + k_3 + extra_tuple\n",
    "running_df = pd.DataFrame()\n",
    "for id, item in k:\n",
    "    running_df = pd.concat([running_df, pd.concat([pd.DataFrame([id]), pd.DataFrame.from_dict(item)], axis=1)], join='outer')\n",
    "f_1.close()\n",
    "f_2.close()\n",
    "f_3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines of code expand nested dict data into their own columns\n",
    "running_df = pd.concat([running_df.drop(columns=['data']), running_df['data'].apply(pd.Series)], axis=1)\n",
    "running_df = pd.concat([running_df.drop(columns=['weather']), running_df['weather'].apply(lambda x: x[0]).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dropped columns that we immediately identified as being unsuitable for our statistical analysis purposes.\n",
    "# This was due to either irrelevancy or missing values.\n",
    "drop_list = ['lat', 'lon', 'timezone', 'timezone_offset', 'dt', 'sunrise', 'sunset', 'icon', 'snow', 'id', 'wind_gust']\n",
    "running_df = running_df.drop(columns=drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming messy columns from expansion of dict data\n",
    "weather_renamer = {0:'ID', 'main':'weather', 'description':'weather_description'}\n",
    "weather_df = running_df.rename(columns=weather_renamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming all columns for ease of use further on\n",
    "new_names = ['ID', 'lat', 'lon', 'start_date', 'incident name', 'incident no.', 'fully_contained', 'imt_type', 'geographic_area', 'new', 'imsr_post_date', 'irwin_id', 'irwin_discovery_time', 'most_recent_date', 'occurrence', 'size', 'nwcg_identifier', 'x','y']\n",
    "fire_renamer = {k:v for k,v in zip(df.columns, new_names)}\n",
    "fire_df = df.rename(columns=fire_renamer)\n",
    "# Dropping unsuitable columns present in the original fire data\n",
    "fire_drop_list = ['incident no.', 'imt_type', 'imsr_post_date', 'irwin_id', 'irwin_discovery_time', 'occurrence', 'most_recent_date', 'nwcg_identifier', 'x', 'y']\n",
    "fire_df = fire_df.drop(columns=fire_drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging together the weather and wildfire datasets, saving the results\n",
    "full_df_raw = fire_df.merge(weather_df, on='ID', how='left')\n",
    "# data in the 'rain' column is in an unsuitable format\n",
    "full_df_raw['rain'] = full_df_raw['rain'].apply(lambda x: x if isinstance(x, float) else x['1h'])\n",
    "full_df_raw.to_csv('data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
